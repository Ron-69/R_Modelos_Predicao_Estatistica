# R_Modelos_Predicao_Estatistica
Notebooks R explorando e implementando algoritmos de Machine Learning para Regress√£o e Classifica√ß√£o, utilizando o ecossistema Tidyverse e pacotes como caret e e1071.

# üìö Resumo de Modelos Preditivos em Machine Learning

Este reposit√≥rio cont√©m Notebooks (em R ou Python) que demonstram a implementa√ß√£o e o uso de diversos algoritmos de Machine Learning para resolver problemas de **Regress√£o** (previs√£o de valores quantitativos) e **Classifica√ß√£o** (previs√£o de categorias).

## üìà Modelos para Problemas de Regress√£o (Predi√ß√£o de Valores Quantitativos)

O objetivo principal √© prever uma vari√°vel de sa√≠da **cont√≠nua** ou **quantitativa** (ex: pre√ßo, temperatura).

| Categoria | Modelo | Descri√ß√£o |
| :--- | :--- | :--- |
| **Base** | **Regress√£o Linear** | Modelo inicial que assume uma rela√ß√£o linear. |
| **Base** | **Regress√£o Polinomial** | Usa transforma√ß√µes para modelar rela√ß√µes n√£o lineares. |
| **Regulariza√ß√£o** | **Regress√£o Ridge (L2)** | Penaliza coeficientes para evitar *overfitting* (encolhe, n√£o zera). |
| **Regulariza√ß√£o** | **Regress√£o Lasso (L1)** | Penaliza coeficientes e pode zer√°-los, servindo como **sele√ß√£o de *features***. |
| **Regulariza√ß√£o** | **Elastic Net** | Combina√ß√£o das penalidades Ridge e Lasso. |
| **Ensemble** | **Random Forest (Regress√£o)** | Combina muitas √Årvores de Decis√£o para alta precis√£o e redu√ß√£o de vari√¢ncia. |
| **Ensemble** | **Gradient Boosting Machines (GBM)** | Constr√≥i √°rvores sequencialmente, corrigindo erros (e.g., XGBoost, LightGBM). |
| **Dist√¢ncia** | **Support Vector Regression (SVR)** | Encontra um hiperplano que minimiza o erro dentro de uma margem ($\epsilon$). |
| **Dist√¢ncia** | **K-Nearest Neighbors (KNN Regress√£o)** | Prev√™ o valor como a m√©dia dos $K$ vizinhos mais pr√≥ximos. |

---
## ‚öñÔ∏è An√°lise Comparativa dos Modelos de Regress√£o Base

A transi√ß√£o da Regress√£o Linear Simples para a M√∫ltipla ilustra como a inclus√£o de *features* adicionais (como Horsepower, `hp`) altera a interpreta√ß√£o e a precis√£o do modelo, especialmente devido √† **multicolinearidade** existente entre as vari√°veis.

### Comparativo de M√©tricas (MPG vs. Peso e HP)

| M√©trica | Simples (`mpg ~ wt`) | M√∫ltiplo (`mpg ~ wt + hp`) | Interpreta√ß√£o da Mudan√ßa |
| :--- | :--- | :--- | :--- |
| **R-quadrado Ajustado** | 0.7446 | **0.8148** | **Melhoria de Ajuste:** O modelo m√∫ltiplo explica cerca de **7% a mais** da vari√¢ncia em MPG. |
| **RSE (Erro Residual)** | 3.046 | **2.593** | **Aumento da Precis√£o:** O erro m√©dio de previs√£o caiu em $\approx 0.45$ unidades, tornando o modelo mais preciso. |
| **Coeficiente do Peso (`wt`)** | **-5.3445** | **-3.87783** | **Multicolinearidade:** O impacto negativo do Peso diminuiu. Isso ocorre porque o Horsepower (`hp`), que √© correlacionado com o Peso, agora "explica" parte da redu√ß√£o no MPG. |
| **P-valor do `hp`** | N/A | $0.00145$ | **Signific√¢ncia:** O Horsepower √© um preditor estatisticamente significativo de MPG, mesmo **ap√≥s** a influ√™ncia do Peso ter sido contabilizada. |

### Conclus√£o da Compara√ß√£o

O modelo de Regress√£o M√∫ltipla √© **estatisticamente superior** ao modelo simples. Ele n√£o s√≥ fornece um ajuste superior (maior R-quadrado Ajustado e menor Erro Residual), mas tamb√©m oferece uma **interpreta√ß√£o mais precisa** do efeito isolado de cada *feature* no consumo de combust√≠vel (o princ√≠pio *ceteris paribus*).

## üìê An√°lise de Forma Funcional: Linear vs. Polinomial

Esta se√ß√£o compara o modelo Polinomial de 2¬∫ Grau (`mpg ~ hp + hp^2`) com o modelo de Regress√£o M√∫ltipla que foi o melhor ajuste linear (`mpg ~ wt + hp`), a fim de determinar a melhor forma de modelar a rela√ß√£o.

### Comparativo de Desempenho (R-quadrado Ajustado e Erro)

| M√©trica | Polinomial ($\text{hp} + \text{hp}^2$) | M√∫ltipla ($\text{wt} + \text{hp}$) | Compara√ß√£o/Conclus√£o |
| :--- | :--- | :--- | :--- |
| **R-quadrado Ajustado** | 0.7393 | **0.8148** | **Melhor Ajuste:** O modelo M√∫ltiplo explica significativamente mais vari√¢ncia. |
| **RSE (Erro Residual)** | 3.077 | **2.593** | **Maior Precis√£o:** O modelo M√∫ltiplo tem um erro de previs√£o consideravelmente menor. |
| **P-valor do $\text{I(hp}^2)$** | $\mathbf{0.000189}$ | N/A | **Validade da Curvatura:** A signific√¢ncia estat√≠stica confirma que a rela√ß√£o **n√£o √© linear**, embora a forma curvil√≠nea n√£o seja a que melhor prediz o alvo. |

### Conclus√£o sobre a Modelagem

1.  **A Rela√ß√£o √© N√£o-Linear:** O termo $\text{I(hp}^2)$ ser estatisticamente significativo prova que a rela√ß√£o entre $\text{HP}$ e $\text{MPG}$ tem uma **curvatura**.
2.  **O Melhor Modelo √© M√∫ltiplo:** Apesar de a forma ser curva, a inclus√£o de um segundo *feature* linear ($\text{wt}$) no modelo **M√∫ltiplo** resultou no **melhor ajuste geral** (maior $\text{R-quadrado Ajustado}$ e menor $\text{RSE}$).
3.  **Implica√ß√£o:** Para o *dataset* `mtcars`, a **combina√ß√£o de features independentes** ($\text{wt}$ e $\text{hp}$) √© mais eficaz para reduzir o erro de previs√£o do que a tentativa de modelar a curvatura de um √∫nico *feature* ($\text{hp}$).

---

## üè∑Ô∏è Modelos para Problemas de Classifica√ß√£o (Predi√ß√£o de Classes/Categorias)

O objetivo principal √© prever uma vari√°vel de sa√≠da **categ√≥rica** ou **discreta** (ex: Sim/N√£o, Classe A/B/C).

| Categoria | Modelo | Descri√ß√£o |
| :--- | :--- | :--- |
| **Probabil√≠stico** | **Regress√£o Log√≠stica** | Calcula a probabilidade de um dado pertencer a uma classe usando a fun√ß√£o Logit/Sigmoide. |
| **Probabil√≠stico** | **Naive Bayes** | Baseado no Teorema de Bayes; assume independ√™ncia ing√™nua dos *features*. |
| **Probabil√≠stico** | **Linear Discriminant Analysis (LDA)** | Encontra uma combina√ß√£o linear de *features* que melhor separa as classes. |
| **Margem** | **Support Vector Machines (SVM)** | Encontra o hiperplano que maximiza a margem entre as classes. |
| **Dist√¢ncia** | **K-Nearest Neighbors (KNN Classifica√ß√£o)** | Classifica um dado pela classe mais comum entre seus $K$ vizinhos pr√≥ximos. |
| **√Årvore** | **√Årvores de Decis√£o** | Cria regras de decis√£o sequenciais. |
| **Ensemble** | **Random Forest (Classifica√ß√£o)** | Vota√ß√£o de m√∫ltiplas √Årvores de Decis√£o para a classifica√ß√£o final. |
| **Ensemble** | **Gradient Boosting Machines (GBM)** | Constr√≥i preditores sequencialmente para alta precis√£o. |
